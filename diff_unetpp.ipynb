{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9027e1fd",
   "metadata": {},
   "source": [
    "# UNet++ on Frame-Difference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a081ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_ROOT = Path('.')\n",
    "DIFF_DIR = DATA_ROOT / 'd_images'\n",
    "MASK_DIR = DATA_ROOT / 'd_masks'\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 20\n",
    "AUGMENT_TRAIN = True\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b5c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 6337\n",
      "train: 4761 samples from 60 sequences\n",
      "val: 843 samples from 13 sequences\n",
      "test: 733 samples from 14 sequences\n"
     ]
    }
   ],
   "source": [
    "def load_pairs(diff_dir: Path, mask_dir: Path):\n",
    "    pairs = []\n",
    "    for diff_path in sorted(diff_dir.glob('*.npy')):\n",
    "        base = diff_path.stem.replace('_diff', '')\n",
    "        mask_path = mask_dir / f\"{base}_diff.png\"\n",
    "        if mask_path.exists():\n",
    "            seq = diff_path.name[:6]\n",
    "            pairs.append(dict(diff=diff_path, mask=mask_path, sequence=seq))\n",
    "    return pd.DataFrame(pairs)\n",
    "\n",
    "pairs_df = load_pairs(DIFF_DIR, MASK_DIR)\n",
    "print('Total pairs:', len(pairs_df))\n",
    "if len(pairs_df) == 0:\n",
    "    raise RuntimeError('No diff/mask pairs found. Run build_diff_dataset.py first.')\n",
    "\n",
    "seqs = pairs_df['sequence'].unique()\n",
    "train_seq, temp_seq = train_test_split(seqs, test_size=0.30, random_state=SEED, shuffle=True)\n",
    "val_seq, test_seq = train_test_split(temp_seq, test_size=0.50, random_state=SEED, shuffle=True)\n",
    "\n",
    "splits = {\n",
    "    'train': pairs_df[pairs_df['sequence'].isin(train_seq)].reset_index(drop=True),\n",
    "    'val': pairs_df[pairs_df['sequence'].isin(val_seq)].reset_index(drop=True),\n",
    "    'test': pairs_df[pairs_df['sequence'].isin(test_seq)].reset_index(drop=True),\n",
    "}\n",
    "for name, df in splits.items():\n",
    "    print(f\"{name}: {len(df)} samples from {df['sequence'].nunique()} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf602a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([4, 1, 512, 512]) torch.Size([4, 1, 512, 512])\n",
      "val torch.Size([4, 1, 512, 512]) torch.Size([4, 1, 512, 512])\n",
      "test torch.Size([4, 1, 512, 512]) torch.Size([4, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "class DiffDataset(Dataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        diff = np.load(row['diff']).astype(np.float32)\n",
    "        mask = cv2.imread(str(row['mask']), cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(row['mask'])\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "\n",
    "        diff_norm = diff / 255.0\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                diff_norm = np.flip(diff_norm, axis=1).copy()\n",
    "                mask = np.flip(mask, axis=1).copy()\n",
    "            if random.random() < 0.5:\n",
    "                diff_norm = np.flip(diff_norm, axis=0).copy()\n",
    "                mask = np.flip(mask, axis=0).copy()\n",
    "\n",
    "        diff_tensor = torch.from_numpy(diff_norm).unsqueeze(0)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "        return diff_tensor, mask_tensor\n",
    "\n",
    "data_loaders = {\n",
    "    'train': DataLoader(DiffDataset(splits['train'], augment=AUGMENT_TRAIN), batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS),\n",
    "    'val': DataLoader(DiffDataset(splits['val']), batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS),\n",
    "    'test': DataLoader(DiffDataset(splits['test']), batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS),\n",
    "}\n",
    "for name, loader in data_loaders.items():\n",
    "    x, y = next(iter(loader))\n",
    "    print(name, x.shape, y.shape)\n",
    "\n",
    "print(f\"Train/val/test samples: {len(splits['train'])}/{len(splits['val'])}/{len(splits['test'])}\")\n",
    "print(f\"Batch sizes: train={TRAIN_BATCH_SIZE}, val/test={VAL_BATCH_SIZE}, num_workers={NUM_WORKERS}\")\n",
    "print(f\"Hyperparams: epochs={NUM_EPOCHS}, lr={LEARNING_RATE}, augment_train={AUGMENT_TRAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63e799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 9.159105 M\n"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=1, filters=(32, 64, 128, 256, 512)):\n",
    "        super().__init__()\n",
    "        f = filters\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = ConvBlock(in_channels, f[0])\n",
    "        self.conv1_0 = ConvBlock(f[0], f[1])\n",
    "        self.conv2_0 = ConvBlock(f[1], f[2])\n",
    "        self.conv3_0 = ConvBlock(f[2], f[3])\n",
    "        self.conv4_0 = ConvBlock(f[3], f[4])\n",
    "\n",
    "        self.conv0_1 = ConvBlock(f[0] + f[1], f[0])\n",
    "        self.conv1_1 = ConvBlock(f[1] + f[2], f[1])\n",
    "        self.conv2_1 = ConvBlock(f[2] + f[3], f[2])\n",
    "        self.conv3_1 = ConvBlock(f[3] + f[4], f[3])\n",
    "\n",
    "        self.conv0_2 = ConvBlock(f[0] * 2 + f[1], f[0])\n",
    "        self.conv1_2 = ConvBlock(f[1] * 2 + f[2], f[1])\n",
    "        self.conv2_2 = ConvBlock(f[2] * 2 + f[3], f[2])\n",
    "\n",
    "        self.conv0_3 = ConvBlock(f[0] * 3 + f[1], f[0])\n",
    "        self.conv1_3 = ConvBlock(f[1] * 3 + f[2], f[1])\n",
    "\n",
    "        self.conv0_4 = ConvBlock(f[0] * 4 + f[1], f[0])\n",
    "\n",
    "        self.final = nn.Conv2d(f[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0_0 = self.conv0_0(x)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], dim=1))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], dim=1))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], dim=1))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], dim=1))\n",
    "\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], dim=1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], dim=1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], dim=1))\n",
    "\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], dim=1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], dim=1))\n",
    "\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], dim=1))\n",
    "\n",
    "        logits = self.final(x0_4)\n",
    "        return logits\n",
    "\n",
    "model = UNetPlusPlus().to(device)\n",
    "print('Model params:', sum(p.numel() for p in model.parameters()) / 1e6, 'M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acce8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(logits, targets, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    numerator = 2 * (probs * targets).sum(dim=(1, 2, 3)) + eps\n",
    "    denominator = probs.sum(dim=(1, 2, 3)) + targets.sum(dim=(1, 2, 3)) + eps\n",
    "    loss = 1 - (numerator / denominator)\n",
    "    return loss.mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "num_epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6b89b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     18\u001b[0m train_history\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val = float('inf')\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_bar = tqdm(data_loaders['train'], desc=f\"Epoch {epoch}/{num_epochs} [train]\", leave=False)\n",
    "    for inputs, targets in train_bar:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = dice_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "    epoch_loss = running_loss / len(data_loaders['train'].dataset)\n",
    "    train_history.append(epoch_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_bar = tqdm(data_loaders['val'], desc=f\"Epoch {epoch}/{num_epochs} [val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = dice_loss(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "    val_loss /= max(1, len(data_loaders['val'].dataset))\n",
    "    val_history.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), 'unetpp_best.pt')\n",
    "    print(f\"Epoch {epoch:02d} | train loss {epoch_loss:.4f} | val loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519127d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "if Path('unetpp_best.pt').exists():\n",
    "    model.load_state_dict(torch.load('unetpp_best.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "per_sample_metrics = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(data_loaders['test'], desc='[test]', leave=False)\n",
    "    for inputs, targets in test_bar:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model(inputs)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        all_preds.append(probs.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "        inter = (preds * targets).sum(dim=(1,2,3))\n",
    "        union = ((preds + targets) > 0).float().sum(dim=(1,2,3))\n",
    "        dice = (2 * inter + 1e-6) / (preds.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + 1e-6)\n",
    "        iou = (inter + 1e-6) / (union + 1e-6)\n",
    "        for d, j in zip(dice.cpu().numpy(), iou.cpu().numpy()):\n",
    "            per_sample_metrics.append(dict(dice=d, iou=j))\n",
    "\n",
    "probs_flat = np.concatenate([p.reshape(-1) for p in all_preds])\n",
    "labels_flat = np.concatenate([t.reshape(-1) for t in all_targets])\n",
    "mask = (labels_flat.max() != labels_flat.min())\n",
    "auc = roc_auc_score(labels_flat, probs_flat) if mask else None\n",
    "\n",
    "results = {\n",
    "    'dice_mean': float(np.mean([m['dice'] for m in per_sample_metrics])),\n",
    "    'iou_mean': float(np.mean([m['iou'] for m in per_sample_metrics])),\n",
    "    'auc': float(auc) if auc is not None else None,\n",
    "    'samples': len(per_sample_metrics)\n",
    "}\n",
    "print('Test metrics:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
